# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gMttDQWH1HzVFqNkzcJEzG10RXiHCm-n
"""

!pip install gtts

!pip install easyocr

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
import numpy as np
from tensorflow.keras.applications import VGG16
import easyocr
from IPython.display import display, HTML , Audio
from google.colab import output , drive , auth
from PIL import Image
import base64
import io
import os
import cv2
from gtts import gTTS
import zipfile

zip_path = '/content/medicine_data.zip'
print(f"zip_path: {zip_path}")

if os.path.exists(zip_path):
    print("File exists at the specified path.")
else:
    print("File not found. Please check the path.")
       # If the file doesn't exist, you need to upload or download the correct file to the specified location.

   # Rest of the code for extracting the ZIP file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
       zip_ref.extractall('/content/medicine_data')
       dataset_path = '/content/medicine_data'

# Paths to the dataset directories
train_dir = '/content/medicine_data/medicine_data'
val_dir = '/content/medicine_data/medicine_data'

# Parameters
batch_size = 32
img_height = 224
img_width = 224

# Load the datasets
train_dataset = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

val_dataset = tf.keras.utils.image_dataset_from_directory(
    val_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

# Data augmentation for training
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)


# Rescale validation data
validation_datagen = ImageDataGenerator(rescale=1.0/255)

# Create generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'  # Use 'binary' for binary classification
)


validation_generator = validation_datagen.flow_from_directory(
   val_dir,
   target_size=(224, 224),
   batch_size=32,
   class_mode='categorical'
)

# Load pre-trained VGG16 model without the top layer
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the base model
base_model.trainable = False

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),


    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),


    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Conv2D(256, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Summary of the model
model.summary()

history=model.fit(train_generator,epochs=25,validation_data=validation_generator)

# Unfreeze some layers of the base model
base_model.trainable = True
for layer in base_model.layers[:15]:  # Freeze the first 15 layers
    layer.trainable = False

# Recompile the model with a lower learning rate
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history=model.fit(train_generator,epochs=25,validation_data=validation_generator)

auth.authenticate_user()

# Mount Google Drive
drive.mount('/content/drive',force_remount=True)

# Define path to save model
model_path = "/content/drive/MyDrive/medicine_detection_model.h5"

# Save model
model.save(model_path)
print(f"Model saved to {model_path}")

# to load the model
drive.mount('/content/drive')

model_path = "/content/drive/MyDrive/medicine_detection_model.h5"
model = tf.keras.models.load_model(model_path)

print("Model loaded successfully!")

# accuracy = history.history['accuracy'][-1]
# print(f"Final Training Accuracy: {accuracy}")
loss , accuracy  = model.evaluate(validation_generator)
print(f"Final Validation Loss: {loss}")
print(f"Final Validation Accuracy: {accuracy}")

# List of medicine names to detect
medicines_list = ['Amlodipine', 'Amoxicillin',  'Atorvastatin',
                  'Ciprofloxacin', 'Ibuprofen', 'Lisinopril', 'Losartan',
                  'Metformin', 'Omeprazole', 'Paracetamol', 'Sertraline']

# 2) OCR list (we add "amoxycillin" so it can match text on the box)
ocr_list = medicines_list + ["amoxycillin"]

# Dictionary containing medicine names and their information
medicines_info = {
    "Amlodipine": "Amlodipine is used to treat high blood pressure and chest pain.",
    "Amoxicillin": "Amoxicillin is an antibiotic used to treat bacterial infections.",
    "Amoxycillin": "Amoxycillin is another name for Amoxicillin, used to treat infections.",
    "Atorvastatin": "Atorvastatin helps lower cholesterol and reduce heart disease risk.",
    "Ciprofloxacin": "Ciprofloxacin is an antibiotic used to treat bacterial infections.",
    "Ibuprofen": "Ibuprofen is a pain reliever and reduces inflammation.",
    "Lisinopril": "Lisinopril is used to treat high blood pressure and heart failure.",
    "Losartan": "Losartan helps lower blood pressure and protect the kidneys.",
    "Metformin": "Metformin is used to treat type 2 diabetes by controlling blood sugar.",
    "Omeprazole": "Omeprazole is used to reduce stomach acid and treat acid reflux.",
    "Paracetamol": "Paracetamol is used for fever and pain relief.",
    "Sertraline": "Sertraline is an antidepressant used to treat anxiety and depression."
}

# Convert medicine names to lowercase for case-insensitive comparison
medicines_list = [med.lower() for med in medicines_list]
ocr_list = [med.lower() for med in ocr_list]

# Initialize a list to track boxes that have already been drawn
drawn_boxes = []
audio_file_to_play = None



# Define the target size for image preprocessing
TARGET_SIZE = (224, 224)

# Function to generate and return audio for a detected medicine
def generate_medicine_audio(medicine_name):
    for med in medicines_info:
        if med.lower() in medicine_name.lower():  # Check for partial matches
            text = medicines_info[med]
            tts = gTTS(text=text, lang='en')

            # Save audio file in a directory within /content/
            audio_path = f"/content/audio/{med}.mp3"
            os.makedirs(os.path.dirname(audio_path), exist_ok=True)  # Ensure the directory exists
            tts.save(audio_path)
            return audio_path

# Preprocess the uploaded image
def preprocess_image(image_data, target_size=(224, 224)):
    # Decode Base64 string to bytes
    image_data = base64.b64decode(image_data.split(",")[1])
    # Load image using PIL
    image = Image.open(io.BytesIO(image_data)).convert("RGB")
    # Resize image to the model's input size
    image = image.resize(target_size)
    # Convert image to numpy array
    image_array = np.array(image)
    # Normalize pixel values (0 to 1)
    image_array = image_array / 255.0
    # Add a batch dimension (1, height, width, channels)
    image_array = np.expand_dims(image_array, axis=0)
    return image_array

# Function to handle image upload and prediction
def upload_image(image_data):
    try:
        print("Received image data.")

        # Define the save path
        save_path = "/content/uploaded_image.jpg"

        # Decode Base64 string to bytes
        image_bytes = base64.b64decode(image_data.split(",")[1])
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        image.save(save_path)

        # Ensure the file exists
        if not os.path.exists(save_path):
            raise FileNotFoundError(f"Image not found at {save_path}")

        print(f"Image saved at: {save_path}")

        # Preprocess the image
        print("Preprocessing the image...")
        preprocessed_image = preprocess_image(image_data, TARGET_SIZE)
        print(f"Preprocessed image shape: {preprocessed_image.shape}")

        # Make a prediction
        print("Making a prediction...")
        predictions = model.predict(preprocessed_image)
        print(f"Raw predictions: {predictions}")

        # Decode predictions
        predicted_class_index = np.argmax(predictions[0])  # predictions is a list of lists, we need the inner list
        class_labels = ocr_list  # Use predefined medicine list
        predicted_class_label = class_labels[predicted_class_index]

        # Display the result (Directly show the image and prediction)
        plt.imshow(image)
        plt.axis('off')
        plt.title(f"Predicted Class: {predicted_class_label.capitalize()} (Confidence: {np.max(predictions):.2%})")
        plt.show()

        # Run OCR after uploading
        img = cv2.imread(save_path)

        if img is None:
            raise ValueError(f"Could not load the image at {save_path}")

        # Initialize EasyOCR Reader
        reader = easyocr.Reader(['en'], gpu=False)

        # Perform OCR
        text_ = reader.readtext(img)

        threshold = 0.15

        # Iterate over each detected text
        for t_, t in enumerate(text_):
            bbox, detected_text, score = t  # Extract bounding box, text, and confidence score

            # Normalize text (remove extra spaces, convert to lowercase)
            detected_text_cleaned = detected_text.strip().lower()

            # Print detected text and confidence for debugging
            print(f"Detected Text: {detected_text} | Confidence: {score:.2%}")

            # Check if detected text contains any of the medicine names (partial match)
            for med in ocr_list:
                if med in detected_text_cleaned and score > threshold:
                    print(f"âœ… Medicine Found: {detected_text}")

                    # Convert bounding box points to integers (this should fix the issue with the box)
                    top_left = tuple(map(int, bbox[0]))  # Top-left corner
                    bottom_right = tuple(map(int, bbox[2]))  # Bottom-right corner

                    # Ensure that the box has not been drawn before
                    if (top_left, bottom_right) not in drawn_boxes:
                        # Draw a rectangle around the detected text using the bounding box coordinates
                        cv2.rectangle(img, top_left, bottom_right, (0, 255, 0), 5)

                        # Overlay the detected text on the image
                        cv2.putText(
                            img,  # The image to draw on
                            detected_text,  # The text to display
                            top_left,  # Position of the text
                            cv2.FONT_HERSHEY_COMPLEX, 0.65,  # Font type & scale
                            (255, 0, 0), 2  # Text color & thickness
                        )

                        # Add this box to the list of drawn boxes to avoid duplicates
                        drawn_boxes.append((top_left, bottom_right))

                        # Generate audio for the detected medicine
                        audio_file_to_play = generate_medicine_audio(med)
                        break  # No need to check further once a match is found

        # Convert the BGR image to RGB (as OpenCV loads images in BGR format)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Display the image with boxes
        plt.imshow(img_rgb)
        plt.axis('off')  # Hide axes for a cleaner display
        plt.show()

        # Display the audio button after the image
        if audio_file_to_play:
            display(Audio(audio_file_to_play, autoplay=True))  # Play the generated audio

        return save_path

    except Exception as e:
        print(f"Error during processing: {e}")

# Register the function for JavaScript callback
output.register_callback('notebook.upload_image', upload_image)

# HTML and JavaScript for uploading and handling image files
def upload_image_page():
    upload_html = """
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f7f7f7;
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 600px;
            margin: 50px auto;
            padding: 20px;
            background-color: #ffffff;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            text-align: center;
        }
        h1 {
            font-size: 24px;
            color: #333;
        }
        p {
            font-size: 16px;
            color: #666;
        }
        .upload-btn {
            display: inline-block;
            margin: 20px auto;
            padding: 10px 20px;
            font-size: 14px;
            cursor: pointer;
            border: none;
            border-radius: 5px;
            background-color: #007bff;
            color: #ffffff;
            text-decoration: none;
            text-align: center;
        }
        .upload-btn:hover {
            background-color: #0056b3;
        }
        input[type="file"] {
            display: none;
        }
        label[for="file-upload"] {
            display: inline-block;
            margin: 20px auto;
            padding: 12px 24px;
            font-size: 16px;
            font-weight: bold;
            background-color: #28a745;
            color: white;
            border-radius: 5px;
            cursor: pointer;
            border: none;
            text-align: center;
        }
        label[for="file-upload"]:hover {
            background-color: #218838;
        }
        #remove-button {
            display: none;
            margin: 20px auto;
            padding: 10px 20px;
            font-size: 16px;
            background-color: #dc3545;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            text-align: center;
        }
        #remove-button:hover {
            background-color: #c82333;
        }
    </style>
    <div class="container">
        <h1>Image Upload and Prediction</h1>
        <p>Click the button below to upload an image file:</p>
        <label for="file-upload">Upload File</label>
        <input type="file" id="file-upload" onchange="handleUpload(event)">
        <div id="image-container"></div>
        <button id="remove-button" onclick="removeImage()">Remove Image</button>
    </div>
    <script>
        function handleUpload(event) {
            var file = event.target.files[0];
            if (file) {
                var reader = new FileReader();
                reader.onload = function(e) {
                    var imageContainer = document.getElementById('image-container');
                    imageContainer.innerHTML = ""; // Clear previous image
                    var imgElement = document.createElement("img");
                    imgElement.src = e.target.result;
                    imgElement.style.maxWidth = "100%";
                    imgElement.style.maxHeight = "400px";
                    imageContainer.appendChild(imgElement);

                    // Show Remove button
                    document.getElementById('remove-button').style.display = "inline-block";

                    // Pass the image data to the Python environment
                    google.colab.kernel.invokeFunction('notebook.upload_image', [e.target.result], {});
                };
                reader.readAsDataURL(file);
            }
        }

        function removeImage() {
            // Clear image and hide the Remove button
            document.getElementById('image-container').innerHTML = "";
            document.getElementById('remove-button').style.display = "none";
            alert("Please upload a new image.");

        }
    </script>
    """
    display(HTML(upload_html))

# Show the upload page
upload_image_page()

